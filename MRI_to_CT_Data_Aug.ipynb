{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNmY5R5DGV3-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import random\n",
    "import math\n",
    "import io\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, random_split, Subset,ConcatDataset\n",
    "import time\n",
    "from matplotlib.colors import Normalize\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "import numpy as np\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/scai/mtech/aib222683/scratch/Task2/data'\n",
    "data_path_Train = os.path.join(path,'train')\n",
    "img_path=[os.path.join(data_path_Train, filename) for filename in os.listdir(data_path_Train)]\n",
    "original_dataset=[]\n",
    "c=0\n",
    "for i,data in enumerate(img_path):\n",
    "    image = Image.open(data)\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    image1=to_tensor(image)\n",
    "    if i%3==0:\n",
    "        original_dataset.append(image1)\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(25),\n",
    "    # transforms.GaussianBlur(kernel_size=29, sigma=11),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    # transforms.RandomResizedCrop(224),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "augmentation_transform_gaussian = transforms.Compose([\n",
    "    # transforms.RandomRotation(45),\n",
    "    transforms.GaussianBlur(kernel_size=29, sigma=9),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    # transforms.RandomResizedCrop(224),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    # transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GuFjXTVGWjw"
   },
   "outputs": [],
   "source": [
    "# log_path = os.path.join(\"/content/drive/MyDrive/Task1/logsaving_data\") #Enter the log saving directory here\n",
    "# /home/scai/mtech/aib222677/scratch/Task1/data\n",
    "\n",
    "path1='/home/scai/mtech/aib222683/scratch/Task2/data'\n",
    "# data_path_Train = os.path.join(path,'train') #Enter the train folder directory\n",
    "# data_path_Test = os.path.join(path,'test') #Enter the test folder directory\n",
    "data_path_Train = os.path.join(path1,'train')\n",
    "data_path_Test = os.path.join(path1,'test')\n",
    "\n",
    "# data_path_Train = os.path.dirname('/home/scai/mtech/aib222677/scratch/khushal/Task2/data/train')\n",
    "# data_path_Test = os.path.dirname('/home/scai/mtech/aib222677/scratch/khushal/Task2/data/test')\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "transform_train = transforms.Compose([\n",
    "                                # transforms.Resize((256,512)),\n",
    "                                # transforms.RandomResizedCrop(size=(256,512)),\n",
    "                                # transforms.RandomRotation(degrees=60),\n",
    "                                # transforms.ElasticTransform(),\n",
    "                                # transforms.ColorJitter(),\n",
    "                                # transforms.GaussianBlur(kernel_size=5),\n",
    "                                # transforms.Resize((256,512)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "                                # transforms.functional.adjust_gamma(inpt, gamma[, gain]),\n",
    "                                # transforms.GaussianBlur(kernel_size=5)\n",
    "                                     ])\n",
    "transform= transforms.Compose([\n",
    "    # transforms.Resize((256,512)),\n",
    "                                # transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainImage(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, filename)) and not filename.startswith('.')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedImage(Dataset):\n",
    "    def __init__(self, data,transform1=None,transform2=None,transform3=None):\n",
    "        self.data = data\n",
    "        self.transform1 =transform1\n",
    "        self.transform2=transform2\n",
    "        self.transform3=transform3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image= self.data[index]\n",
    "\n",
    "\n",
    "        random_number = random.randint(1, 3)\n",
    "    \n",
    "        if random_number == 1:\n",
    "           augmented_image = self.transform1(image)\n",
    "           # augmented_dataset.append(augmented_image)\n",
    "\n",
    "            \n",
    "        \n",
    "        elif random_number == 2:\n",
    "           img1=image[:,:,:256]\n",
    "           img2=image[:,:,256:]\n",
    "           img1=transforms.functional.adjust_gamma(img1,gamma=0.8,gain=1)\n",
    "           normal_image=self.transform3(img1)\n",
    "           normal_image2=self.transform3(img2)\n",
    "           img=torch.cat((normal_image, normal_image2), dim=2)\n",
    "           # augmented_dataset.append(img)\n",
    "           image=img\n",
    "            \n",
    "        else:\n",
    "            img1=image[:,:,:256]\n",
    "            img2=image[:,:,256:]\n",
    "            augmented_image = self.transform2(img2)\n",
    "            img=torch.cat((augmented_image, img2), dim=2)\n",
    "            image=img\n",
    "            # augmented_dataset.append(img)\n",
    "\n",
    "\n",
    "        return image\n",
    "\n",
    "augmented_data=AugmentedImage(original_dataset,augmentation_transform,augmentation_transform_gaussian,transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LLLi9DJI2vl"
   },
   "outputs": [],
   "source": [
    "# Load the entire training dataset\n",
    "\n",
    "full_dataset=BrainImage(data_dir=data_path_Train, transform=transform_train)\n",
    "# c=0\n",
    "# for data in full_dataset:\n",
    "#     c+=1\n",
    "#     print(type(data))\n",
    "#     break\n",
    "# print(len(full_dataset))\n",
    "# Split the dataset into training and validation sets (use the first 1000 for training)\n",
    "# train_dataset, _ = random_split(full_dataset, [1000, len(full_dataset) - 1000])\n",
    "# train_dataset=Subset(full_dataset, range(5000))\n",
    "# train_dataset=Subset(full_dataset, range(25000))   # 30 patients\n",
    "# train_dataset=full_dataset\n",
    "\n",
    "train_dataset=full_dataset+augmented_data\n",
    "# Create a data loader for training\n",
    "load_Train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv7QmQJVJ-nS"
   },
   "outputs": [],
   "source": [
    "# Load the entire test dataset\n",
    "full_test_dataset = BrainImage(data_dir=data_path_Test, transform=transform_train)\n",
    "test_dataset=full_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEsnqfLuKHEe",
    "outputId": "044ca266-d90b-42a2-8e1b-9d5b914472ec"
   },
   "outputs": [],
   "source": [
    "def split(img):\n",
    "    return img[:,:,:,:256], img[:,:,:,256:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_x5GZanHxVG"
   },
   "outputs": [],
   "source": [
    "inst_norm = True if batch_size==1 else False  # instance normalization\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride,\n",
    "    padding=padding)\n",
    "\n",
    "\n",
    "def conv_n(in_channels, out_channels, kernel_size, stride=1, padding=0, inst_norm=False):\n",
    "    if inst_norm == True:\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "        stride=stride, padding=padding), nn.InstanceNorm2d(out_channels,\n",
    "        momentum=0.1, eps=1e-5),)\n",
    "    else:\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "        stride=stride, padding=padding), nn.BatchNorm2d(out_channels,\n",
    "        momentum=0.1, eps=1e-5),)\n",
    "\n",
    "def tconv(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0,):\n",
    "    return nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride,\n",
    "    padding=padding, output_padding=output_padding)\n",
    "\n",
    "def tconv_n(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, inst_norm=False):\n",
    "    if inst_norm == True:\n",
    "        return nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n",
    "        stride=stride, padding=padding, output_padding=output_padding),\n",
    "        nn.InstanceNorm2d(out_channels, momentum=0.1, eps=1e-5),)\n",
    "    else:\n",
    "        return nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n",
    "        stride=stride, padding=padding, output_padding=output_padding),\n",
    "        nn.BatchNorm2d(out_channels, momentum=0.1, eps=1e-5),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IdmxqS_Icku"
   },
   "outputs": [],
   "source": [
    "dim_c = 3\n",
    "dim_g = 64\n",
    "\n",
    "# Generator\n",
    "class Gen(nn.Module):\n",
    "    def __init__(self, inst_norm=False):\n",
    "        super(Gen,self).__init__()\n",
    "        self.n1 = conv(dim_c, dim_g, 4, 2, 1)\n",
    "        self.n2 = conv_n(dim_g, dim_g*2, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.n3 = conv_n(dim_g*2, dim_g*4, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.n4 = conv_n(dim_g*4, dim_g*8, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.n5 = conv_n(dim_g*8, dim_g*8, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.n6 = conv_n(dim_g*8, dim_g*8, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.n7 = conv_n(dim_g*8, dim_g*8, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.n8 = conv(dim_g*8, dim_g*8, 4, 2, 1)\n",
    "\n",
    "        self.m1 = tconv_n(dim_g*8, dim_g*8, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.m2 = tconv_n(dim_g*8*2, dim_g*8, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.m3 = tconv_n(dim_g*8*2, dim_g*8, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.m4 = tconv_n(dim_g*8*2, dim_g*8, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.m5 = tconv_n(dim_g*8*2, dim_g*4, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.m6 = tconv_n(dim_g*4*2, dim_g*2, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.m7 = tconv_n(dim_g*2*2, dim_g*1, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.m8 = tconv(dim_g*1*2, dim_c, 4, 2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self,x):\n",
    "        n1 = self.n1(x)\n",
    "        n2 = self.n2(F.leaky_relu(n1, 0.2))\n",
    "        n3 = self.n3(F.leaky_relu(n2, 0.2))\n",
    "        n4 = self.n4(F.leaky_relu(n3, 0.2))\n",
    "        n5 = self.n5(F.leaky_relu(n4, 0.2))\n",
    "        n6 = self.n6(F.leaky_relu(n5, 0.2))\n",
    "        n7 = self.n7(F.leaky_relu(n6, 0.2))\n",
    "        n8 = self.n8(F.leaky_relu(n7, 0.2))\n",
    "        m1 = torch.cat([F.dropout(self.m1(F.relu(n8)), 0.5, training=True), n7], 1)\n",
    "        m2 = torch.cat([F.dropout(self.m2(F.relu(m1)), 0.5, training=True), n6], 1)\n",
    "        m3 = torch.cat([F.dropout(self.m3(F.relu(m2)), 0.5, training=True), n5], 1)\n",
    "        m4 = torch.cat([self.m4(F.relu(m3)), n4], 1)\n",
    "        m5 = torch.cat([self.m5(F.relu(m4)), n3], 1)\n",
    "        m6 = torch.cat([self.m6(F.relu(m5)), n2], 1)\n",
    "        m7 = torch.cat([self.m7(F.relu(m6)), n1], 1)\n",
    "        m8 = self.m8(F.relu(m7))\n",
    "\n",
    "        return self.tanh(m8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eds_dHD5IfYB"
   },
   "outputs": [],
   "source": [
    "dim_d = 64\n",
    "\n",
    "# Discriminator\n",
    "class Disc(nn.Module):\n",
    "    def __init__(self, inst_norm=False):\n",
    "        super(Disc,self).__init__()\n",
    "        self.c1 = conv(dim_c*2, dim_d, 4, 2, 1)\n",
    "        self.c2 = conv_n(dim_d, dim_d*2, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.c3 = conv_n(dim_d*2, dim_d*4, 4, 2, 1, inst_norm=inst_norm)\n",
    "        self.c4 = conv_n(dim_d*4, dim_d*8, 4, 1, 1, inst_norm=inst_norm)\n",
    "        self.c5 = conv(dim_d*8, 1, 4, 1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        xy=torch.cat([x,y],dim=1)\n",
    "        xy=F.leaky_relu(self.c1(xy), 0.2)\n",
    "        xy=F.leaky_relu(self.c2(xy), 0.2)\n",
    "        xy=F.leaky_relu(self.c3(xy), 0.2)\n",
    "        xy=F.leaky_relu(self.c4(xy), 0.2)\n",
    "        xy=self.c5(xy)\n",
    "\n",
    "        return self.sigmoid(xy)\n",
    "\n",
    "def weights_init(z):\n",
    "    cls_name =z.__class__.__name__\n",
    "    if cls_name.find('Conv')!=-1 or cls_name.find('Linear')!=-1:\n",
    "        nn.init.normal_(z.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(z.bias.data, 0)\n",
    "    elif cls_name.find('BatchNorm')!=-1:\n",
    "        nn.init.normal_(z.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(z.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1ffYi1eIi7E"
   },
   "outputs": [],
   "source": [
    "BCE = nn.BCELoss() #binary cross-entropy\n",
    "L1 = nn.L1Loss()\n",
    "L2=nn.MSELoss()\n",
    "#instance normalization\n",
    "Gen_model = Gen(inst_norm).to(device)\n",
    "Disc = Disc(inst_norm).to(device)\n",
    "generator = Gen(inst_norm).to(device)\n",
    "# #optimizers\n",
    "# Gen_optim = optim.Adam(Gen.parameters(), lr=2e-4, betas=(0.5, 0.999), weight_decay=0.35)\n",
    "Disc_optim = optim.Adam(Disc.parameters(), lr=5e-5, betas=(0.5, 0.999))\n",
    "Gen_optim = optim.Adam(Gen_model.parameters(), lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FeQfWrYjKnZn",
    "outputId": "d84608f2-d2e1-4b71-d210-c761e6a2d16d"
   },
   "outputs": [],
   "source": [
    "# fix_con, _ = next(iter(load_Test))|\n",
    "fix_con= next(iter(load_Train))\n",
    "fix_con = fix_con.to(device)\n",
    "fix_X, fix_y = split(fix_con)\n",
    "\n",
    "def compare_batches(batch1, batch2, batch3, title1, title2,title3):\n",
    "    # batch1\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title1)\n",
    "    plt.imshow(np.transpose(vutils.make_grid(batch1, nrow=1, padding=5,\n",
    "    normalize=True).cpu(), (1,2,0)))\n",
    "    # batch2\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title2)\n",
    "    plt.imshow(np.transpose(vutils.make_grid(batch2, nrow=1, padding=5,\n",
    "    normalize=True).cpu(), (1,2,0)))\n",
    "    # third batch\n",
    "    if batch3 is not None:\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(title3)\n",
    "        plt.imshow(np.transpose(vutils.make_grid(batch3, nrow=1, padding=5,\n",
    "        normalize=True).cpu(), (1,2,0)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    fk = Gen_model(fix_X)\n",
    "# compare_batches(fix_X, fk,fix_y, \"input image\", \"prediction\",  \"ground truth\")\n",
    "for j in range(fk.shape[0]):\n",
    "    if j%3==0:\n",
    "        figs=plt.figure(figsize=(10,10))\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"input image\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(fix_X[j], nrow=1, padding=5,\n",
    "        normalize=True).cpu(), (1,2,0)))\n",
    "    \n",
    "        plt.subplot(1,3,2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"generated image\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(fk[j], nrow=1, padding=5,\n",
    "        normalize=True).cpu(), (1,2,0)))\n",
    "    \n",
    "        plt.subplot(1,3,3)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"ground truth\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(fix_y[j], nrow=1, padding=5,\n",
    "        normalize=True).cpu(), (1,2,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "qLH0qjIsIoJi",
    "outputId": "05351d34-f0c9-4d4c-a1f9-ae205b54d496"
   },
   "outputs": [],
   "source": [
    "# # img_list = []\n",
    "# # Disc_losses = Gen_losses = Gen_GAN_losses = Gen_L1_losses = []\n",
    "\n",
    "\n",
    "iter_per_plot = 20\n",
    "epochs = 100\n",
    "L1_lambda = 50.0\n",
    "# ssim_scores = []\n",
    "# mse_scores = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    for i, data in enumerate(load_Train):\n",
    "        start_time = time.time()\n",
    "        size = data.shape[0]\n",
    "\n",
    "        x, y = split(data.to(device))\n",
    "\n",
    "        r_masks = torch.ones(size,1,30,30).to(device)\n",
    "        f_masks = torch.zeros(size,1,30,30).to(device)\n",
    "\n",
    "        # disc\n",
    "        Disc.zero_grad()\n",
    "        #real_patch\n",
    "        r_patch=Disc(y,x)\n",
    "        r_gan_loss=L2(r_patch,r_masks)\n",
    "\n",
    "        fake=Gen_model(x)\n",
    "        #fake_patch\n",
    "        f_patch = Disc(fake.detach(),x)\n",
    "        f_gan_loss=L2(f_patch,f_masks)\n",
    "\n",
    "        Disc_loss = r_gan_loss + f_gan_loss\n",
    "        Disc_loss.backward()\n",
    "        Disc_optim.step()\n",
    "\n",
    "        # gen\n",
    "        Gen_model.zero_grad()\n",
    "        f_patch = Disc(fake,x)\n",
    "        f_gan_loss=L2(f_patch,r_masks)\n",
    "\n",
    "        L1_loss = L1(fake,y)\n",
    "        Gen_loss = f_gan_loss + L1_lambda*L1_loss\n",
    "        Gen_loss.backward()\n",
    "\n",
    "        Gen_optim.step()\n",
    "        end_time = time.time()  # End measuring time for each iteration\n",
    "        elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "\n",
    "        # Print custom training information\n",
    "        # print('(epoch: {:d}, iters: {:d}, time: {:.3f}, data: 0.003) G_GAN: {:.3f} G_L1: {:.3f} D_real: {:.3f} D_fake: {:.3f}'.format(ep, i, elapsed_time, Gen_GAN_losses[-1], Gen_L1_losses[-1], r_patch.mean(), f_patch.mean()))\n",
    "\n",
    "        if (i+1)%iter_per_plot == 0 :\n",
    "\n",
    "            print('Epoch [{}/{}], Step [{}/{}], disc_loss: {:.4f}, gen_loss: {:.4f},Disc(real): {:.2f}, Disc(fake):{:.2f}, gen_loss_gan:{:.4f}, gen_loss_L1:{:.4f}'.format(ep+1, epochs, i+1, len(load_Train), Disc_loss.item(), Gen_loss.item(), r_patch.mean(), f_patch.mean(), f_gan_loss.item(), L1_loss.item()))\n",
    "\n",
    "            # Gen_losses.append(Gen_loss.item())\n",
    "            # Disc_losses.append(Disc_loss.item())\n",
    "            # Gen_GAN_losses.append(f_gan_loss.item())\n",
    "            # Gen_L1_losses.append(L1_loss.item())\n",
    "    if (ep+1)%5==0:\n",
    "        with torch.no_grad():\n",
    "            Gen_model.eval()\n",
    "            for data in load_Train:\n",
    "                # print(data.shape)\n",
    "                x, y = split(data.to(device))\n",
    "                fake = Gen_model(x)\n",
    "                for j in range(fake.shape[0]):\n",
    "                    if j%10==0:\n",
    "                        figs=plt.figure(figsize=(10,10))\n",
    "            \n",
    "                        plt.subplot(1,3,1)\n",
    "                        plt.axis(\"off\")\n",
    "                        plt.title(\"input image\")\n",
    "                        plt.imshow(np.transpose(vutils.make_grid(x[j], nrow=1, padding=5,\n",
    "                        normalize=True).cpu(), (1,2,0)))\n",
    "                    \n",
    "                        plt.subplot(1,3,2)\n",
    "                        plt.axis(\"off\")\n",
    "                        plt.title(\"generated image\")\n",
    "                        plt.imshow(np.transpose(vutils.make_grid(fake[j], nrow=1, padding=5,\n",
    "                        normalize=True).cpu(), (1,2,0)))\n",
    "                    \n",
    "                        plt.subplot(1,3,3)\n",
    "                        plt.axis(\"off\")\n",
    "                        plt.title(\"ground truth\")\n",
    "                        plt.imshow(np.transpose(vutils.make_grid(y[j], nrow=1, padding=5,\n",
    "                        normalize=True).cpu(), (1,2,0)))\n",
    "        \n",
    "                    \n",
    "                # compare_batches(x, fake, y,\"input images\", \"predicted images\",  \"ground truth\")\n",
    "                break\n",
    "            Gen_model.train()\n",
    "        \n",
    "        if ep+1==epochs:\n",
    "            torch.save(Gen_model.state_dict(), 'model18.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # Set the batch size to 1 to get a single image in each iteration\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "c=0\n",
    "generator.load_state_dict(torch.load('model18.pth', map_location=device))\n",
    "\n",
    "for i,data in enumerate (test_loader):\n",
    "    if i%155==0:\n",
    "        c+=1\n",
    "        t_batch=data\n",
    "        t_x, t_y = split(t_batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generator.eval()\n",
    "            fk= generator(t_x.to(device))\n",
    "            t_y=t_y.to(device)\n",
    "            figs=plt.figure(figsize=(10,10))\n",
    "\n",
    "            plt.subplot(1,4,1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"input image\")\n",
    "            \n",
    "            plt.imshow(np.rot90(np.transpose(vutils.make_grid(t_x[0], nrow=1, padding=5,\n",
    "            normalize=True).cpu(), (1,2,0)),k=-1))\n",
    "            \n",
    "            \n",
    "            plt.subplot(1,4,2)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"generated image\")\n",
    "            plt.imshow(np.rot90(np.transpose(vutils.make_grid(fk[0], nrow=1, padding=5,\n",
    "            normalize=True).cpu(), (1,2,0)),k=-1))\n",
    "            # plt.colorbar()\n",
    "            \n",
    "            plt.subplot(1,4,3)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"ground truth\")\n",
    "            plt.imshow(np.rot90(np.transpose(vutils.make_grid(t_y[0], nrow=1, padding=5,\n",
    "            normalize=True).cpu(), (1,2,0)),k=-1))\n",
    "            # plt.colorbar()\n",
    "            # print(t_y[j].is_cuda)\n",
    "            # print(fk[j].is_cuda)\n",
    "            plt.subplot(1,4,4)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"error map\")\n",
    "    \n",
    "        \n",
    "            data_eg=np.transpose(np.abs((t_y[0]-fk[0]).cpu().numpy()),(1,2,0))\n",
    "            normalized_data=Normalize()(data_eg)\n",
    "            normalized_data=np.mean(normalized_data, axis=2, keepdims=True)\n",
    "            # print(normalized_data.shape)\n",
    "            plt.imshow(np.rot90((normalized_data),k=-1),cmap='jet')\n",
    "\n",
    "            if c==10:\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZeB82-0cXv1"
   },
   "outputs": [],
   "source": [
    "def calculate_psnr(true_image, fake_image, data_range):\n",
    "    mse = np.mean((true_image - fake_image) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 20 * np.log10(data_range / np.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "batch_size = 1  # Set the batch size to 1 to get a single image in each iteration\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "error_list = []\n",
    "structural_similarity = []\n",
    "psnr_list = []\n",
    "\n",
    "# Now you can iterate over it with random samples\n",
    "for i, data in enumerate(test_loader):\n",
    "    t_batch = data\n",
    "    t_x, t_y = split(t_batch)\n",
    "    with torch.no_grad():\n",
    "        generator.eval()\n",
    "        fk_batch = generator(t_x.to(device))\n",
    "    # if i%10==0:\n",
    "    #     compare_batches(t_x, fk_batch, t_y, \"input images\", \"predicted images\",\"ground truth\")\n",
    "    t_y = t_y.numpy()\n",
    "    fk_batch = fk_batch.cpu().detach().numpy()\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((t_y - fk_batch) ** 2))\n",
    "    error_list.append(rmse)\n",
    "\n",
    "    # Calculate SSIM\n",
    "    t_y = np.transpose(t_y.squeeze(), (1, 2, 0))\n",
    "    fk_batch = np.transpose(fk_batch.squeeze(), (1, 2, 0))\n",
    "    ssi = ssim(t_y, fk_batch, multichannel=True, win_size=3, data_range=2.0)\n",
    "    structural_similarity.append(ssi)\n",
    "\n",
    "    # Calculate PSNR\n",
    "    psnr_value = calculate_psnr(t_y, fk_batch, data_range=2.0)\n",
    "    psnr_list.append(psnr_value)\n",
    "\n",
    "# Convert lists to numpy arrays for statistics calculation\n",
    "error_list = np.array(error_list)\n",
    "structural_similarity_np = np.array(structural_similarity)\n",
    "psnr_np = np.array(psnr_list)\n",
    "\n",
    "# Print statistics\n",
    "print('Mean RMSE error:', np.mean(error_list))\n",
    "print('Max RMSE error:', np.max(error_list))\n",
    "print('Min RMSE error:', np.min(error_list))\n",
    "print('Mean SSIM:', np.mean(structural_similarity_np))\n",
    "print('Min SSIM:', np.min(structural_similarity_np))\n",
    "print('Max SSIM:', np.max(structural_similarity_np))\n",
    "print('Mean PSNR:', np.mean(psnr_np))\n",
    "print('Min PSNR:', np.min(psnr_np))\n",
    "print('Max PSNR:', np.max(psnr_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
